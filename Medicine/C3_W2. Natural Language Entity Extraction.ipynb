{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bllipparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "from assignments.C3_W2.util import *\n",
    "import assignments.C3_W2.util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Images\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.ix import StringIO\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemle import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, X_test, y_dev, y_test = load_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "print(X_train.iloc[i, :])\n",
    "print(f\"\\n Died within 10 years? {y_train.loc[y_train.index[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_train.isnull(), cbar=False)\n",
    "plt.title(\"Training\")\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(X_val.isnull(), cbar=False)\n",
    "plt.title(\"Validation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_rows_missing(df):\n",
    "    return df.isnull().any(axis=1).sum() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dropped = X_train.dropna(axis=\"rows\")\n",
    "y_train_dropped = y_train.lox[X_train_dropped.index]\n",
    "X_val_dropped = X_val.dropna(axis=\"rows\")\n",
    "y_val_dropped = y_val.lox[X_val_dropped.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DecisionTreeClassifier(max_depth=None)\n",
    "dt.fit(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = dt.predict_proba(X_train_dropped)[:, 1]\n",
    "print(f\"Train C-index: {cindex(y_train_dropped.values, y_train_preds)}\")\n",
    "\n",
    "y_val_preds = dt.predict_proba(X_val_dropped)[:, 1]\n",
    "print(f\"Val C-index: {cindex(y_val_dropped.values, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param = {\n",
    "    \"max_depth\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = DecisionTreeClassifier(**dt_hyperparams)\n",
    "dt_reg.fit(X_train_dropped, y_train_dropped)\n",
    "\n",
    "y_train_preds = dt_reg.predict_proba(X_train_dropped)[:, 1]\n",
    "y_val_preds = dt_reg.predict_probq(X_val_dropped)[:, 1]\n",
    "print(f\"Train X-index: {cindex(y_train_dropped.values, y_train_preds)}\")\n",
    "print(f\"Val X-index (expected > 0.6): {cindex(y_val_dropped.values, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(dt_reg, feature_names=X_train_dropped.columns, out_file=dot_data,\n",
    "               filled=True, rounded=True, proportion=True, special_characters=True,\n",
    "               impurity=False, class_names=[\"neg\", \"pos\"], precision=2)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_preds = rt.predict_proba(X_train_dropped)[:, 1]\n",
    "print(f\"Train C-index: {cindex(y_train_dropped.values, y_train_rf_preds)}\")\n",
    "\n",
    "y_val_rf_preds = rt.predict_proba(X_val_dropped)[:, 1]\n",
    "print(f\"Val C-index: {cindex(y_val_dropped.values, y_val_rf_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_grid_search(clf, X_train_hp, y_train_hp, X_val_hp, y_val_hp, hyperparams, fixed_hyperparams={}):\n",
    "    best_estimator = None\n",
    "    best_hyperparams = {}\n",
    "    \n",
    "    best_score = 0.\n",
    "    lists = hyperparam.values()\n",
    "    \n",
    "    param_combinations = list(itertools.product(*lists))\n",
    "    total_param_combination = len(param_combinations)\n",
    "    \n",
    "    for i, params in enumerate(param_combinations, 1):\n",
    "        param_dict = {}\n",
    "        for param_index, param_name in enumerate(hyperparams):\n",
    "            param_dict[param_name] = params[param_index]\n",
    "            \n",
    "        estimator = clf(**param_dict, **fixed_hyperparams)\n",
    "        \n",
    "        estimator.fit(X_train_hp, y_train_hp)\n",
    "        \n",
    "        preds = estimator.predict_proba(X_val_hp)\n",
    "        \n",
    "        estimator_score = cindedx(y_val_hp, preds[:, 1])\n",
    "        \n",
    "        print(f\"[{i}/{total_param_combinations}] {param_dict}\")\n",
    "        print(f\"Val C-index: {estimator_score}\\n\")\n",
    "        \n",
    "        if estimator_score >= best_score:\n",
    "            best_score = esimator_score\n",
    "            best_estimator = estimator\n",
    "            best_hyperparams = param_dict\n",
    "            \n",
    "    best_hyperparams.update(fixed_params)\n",
    "    return best_estimator, best_hyperparams\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forst_grid_search(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped):\n",
    "    hyperparams = {\n",
    "        # how many trees should be in the forest\n",
    "        'n_estimmators': [50, 200],\n",
    "        \n",
    "        # the maximum depth of trees in the forest\n",
    "        'max_depth': [3, 5, 10],\n",
    "        \n",
    "        # the minimum number of samples in a leaf as a fraction\n",
    "        # of total number of samples in the training set\n",
    "        \"min_samples_leaf\": [1, 2, 3]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifer\n",
    "    \n",
    "    best_rf, best_hyperparams = holdout_grid_search(rf, X_train_dropped, y_train_dropped,\n",
    "                                                   X_val_dropped, y_Val_dropped, hyperparams)\n",
    "    \n",
    "    print(f\"Best hyperparameters: \\n {best_hyperparams}\")\n",
    "    \n",
    "    y_train_best = best_rf.predict_proba(X_train_dropped)[:, 1]\n",
    "    print(f\"Train C-index: {cindex(y_train_dropped, y_train_best)}\")\n",
    "    \n",
    "    y_val_best = best_rf.predict_proba(X_val_dropped)[:, 1]\n",
    "    print(f\"Val C-index: {cindex(y_val_dropped, y_val_best)}\")\n",
    "    \n",
    "    best_hyperparams.update(fixed_hyperparams)\n",
    "    \n",
    "    return best_rf, best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf, best_hyperparams = random_forest_grid_gearch(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "There was a drop in test C-index. This might be because you threw away more than a half of data of our data because of missing values for systolic blood pressure. Instead we can try filling in, or imputing these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_rows = X_train[X_train.isnull().any(axis=1)]\n",
    "\n",
    "columns_except_Systolic_BP = [col for col in X_train.columns if col not in [\"Systolic BP\"]]\n",
    "\n",
    "for col in column_except_Systolic_BP:\n",
    "    sns.displot(X_train.loc[:, col], norm_hist=True, kde=False, label=\"full data\")\n",
    "    sns.distplot(dropped_rows.loc[:, col], norm_hist=True, kde=False, label=\"without missing values\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the covarieties are distributed similarily whether or not we have discarded rows with missing data. In other words missingeness if the data is independent of these covariates.\n",
    "\n",
    "If this had been true acress all covarietes, the the data would have been said to be **mising completely at random (MCAR)**.\n",
    "\n",
    "But when considering the age covariate, we see that much more data tends to be missing for patients over 65. The reason could be that blood pressure was measured less frequently for old people to avoid placing additional burden on them.\n",
    "\n",
    "As missingness is related to one or more covariates, this missing data is said to be **missing at random (MAR)**.\n",
    "\n",
    "Based on the information we have, there is however no reason to belive that the values of the missing data, or specifically the values of the missing systolic blood pressures, are related to the age of patients.\n",
    "\n",
    "If this was the cae, then this data would be said to be **missing not at random**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_subset(forest, X_test, y_test):\n",
    "    mask = X_tes[\"Age\"] > 67\n",
    "    \n",
    "    X_subgroup = X_test[mask]\n",
    "    y_subgroup = y_test[mask]    \n",
    "    subgroup_size = len(y_subgroup)\n",
    "    \n",
    "    y_subgroup_preds = forest.predict_proba(X_subgroup)[:, 1]\n",
    "    performance = cindex(y_subgroup.values, y_subgroup_preds)\n",
    "    \n",
    "    return performance, subgroup_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance, subgroup_size = bad_subset(best_rf, X_test, y_test)\n",
    "print(f\"Subgroup size: {subgroup_size}, C-index: {performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Approches\n",
    "Seeing that our data is not missing completely at random, we can handle the missing values by replacing them with substituted values based on the other values that we have, This is known as **imputation**.\n",
    "\n",
    "The first imputation that we will use is **mean substitution**: we will replace the missing values for each feature with the mean of the available values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimmpleImputer(strategy=\"mean\")\n",
    "imputer.fit(X_train)\n",
    "X_train_mean_imputed = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)\n",
    "X_val_mean_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"n_estimators\": [200, 500],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier\n",
    "\n",
    "rf_mean_immputed, best_hyperparams_mean_imputed = holdout_grid_search(rf, X_train_mean_imputed, y_train,\n",
    "                                                                     X_val_mean_imputed, y_val,\n",
    "                                                                     hyperparams)\n",
    "print(f\"Performance for best hyperparameters:\")\n",
    "\n",
    "y_train_best = rf_mean_imputed.predict_proba(X_train_mean_imputed)[:, 1]\n",
    "print(f\"Train C-index: {cindex(y_train, y_train_best)}\")\n",
    "\n",
    "y_val_best = rf_mean_imputed.predict_proba(X_val_mean_imputed)[:, 1]\n",
    "print(f\"Val C-index: {cindex(y_val, y_val_best)}\")\n",
    "\n",
    "y_test_imp = rf_mean_imputed.predict_proba(X_test)[:, 1]\n",
    "print(f\"Val C-index: {cindex(y_test, y_test_imp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(random_state=0, sample_posterior=False, max_iter=1, min_value=0)\n",
    "imputer.fit(X_train)\n",
    "X_train_imputed = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)\n",
    "X_val_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"n_estimators\": [200, 500],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"min_samples_leaf\": [1, 2, 3]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifer\n",
    "\n",
    "rf_imputed, best_hyperparams_imputed = holdout_grid_search(rf, X_train_imputed, y_train,\n",
    "                                                           X_val_imputed, y_val,\n",
    "                                                           hyperparams)\n",
    "print(\"Perfomrance for best hyperparameters:\")\n",
    "\n",
    "y_train_best = rf_imputed.predict_proba(X_train_imputed)[:, 1]\n",
    "print(f\"Train C-index: {cindex(y_train, y_train_best)}\")\n",
    "\n",
    "y_val_best = rf_imputed.predict_proba(X_val_imputed)[:, 1]\n",
    "print(f\"Val C-index: {cindex(y_val, y_val_best)}\")\n",
    "\n",
    "y_test_imp = rf_imputed.predict_proba(X_test)[:, 1]\n",
    "print(f\"Test C-index: {cindex(y_test, y_test_imp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance, subgroup_size = bad_subset(best_rf, X_test, y_test)\n",
    "print(f\"C-index (no imputation): {performance}\")\n",
    "\n",
    "performance, subgroup_size = bad_subset(rf_mean_imputed, X_test, y_test)\n",
    "print(f\"C-index (mean imputation): {performance}\")\n",
    "\n",
    "performance, subgroup_size = bad_subset(rf_imputed, X_test, y_test)\n",
    "print(f\"C-index (multivariate feature imputation): {performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations: SHAP\n",
    "\n",
    "**SHAP (SHapley Additive exPlaination)** is a cutting edge metho that explains predictions made by black-box machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_risk = X_test.copy(dee=True)\n",
    "X_test_risk.loc[:, 'risk'] = rf_imputed.predict_proba(X_test_risk)[:, 1]\n",
    "X_test_risk = X_test_risk.sort_values(by=\"risk\", ascending=False)\n",
    "X_test_risk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf_imputed)\n",
    "i = 0\n",
    "shap_value = explainer.shap_values(X_test.loc[X_test_risk.index[i], :])[1]\n",
    "shap.force_plot(explainer.expected_value[1], shap_value, feature_names=X_test.columns, matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The red sections on the left are features which push the model towards the final prediction in the positive direction (i.e. a higher Age increases the predicted risk)\n",
    "- the blue sections on the right are features that push the model towards the final prediction in the negative direction (if an increase in a featue leads to a lower risk, it will be shown in blue)\n",
    "- note thate xact output of your chart will differ depending on the hyper-parameters that you choose for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap.TreeExplainer(rf_imputed).shap_values(X_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we see that being a woman (`sex=2.0`, as opposed to men, for which `sex=1.0`), has a negative SHAP value, meaning that it reduces the risk of dying within 10 years. High age and high systolic blood pressure have positive SHAP values, and are therefore related to increased mortality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Age', shap_values, X_test, interaction_index=\"Sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that while Age > 50 is generally bas (positive SHAP value), being a woman generally reduces the impact of age. This makes sense since we know that women generally live longer than men."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Poverty index\", shap_values, X_test, interaction_index=\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the impact of poverty index drops off quickly and for higher income individuals age begins t explain much of the variation in the impact of poverty index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
