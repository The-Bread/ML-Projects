{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_vector(params):\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n",
    "        new_vector = np.reshape(params[key], (-1, 1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concenate((theta, new_vector), axis=0)\n",
    "        count += 1\n",
    "    \n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dict(theta):\n",
    "    params = {}\n",
    "    params[\"W1\"] = theta[:20].reshape((5, 4))\n",
    "    params[\"b1\"] = theta[20:25].reshape((5, 1))\n",
    "    params[\"W2\"] = theta[25:40].reshape((3, 5))  \n",
    "    params[\"b2\"] = theta[40:43].reshape((3, 1))\n",
    "    params[\"W3\"] = theta[43:46].reshape((1, 3))    \n",
    "    params[\"b3\"] = theta[46:47].reshape((1, 1))\n",
    "    \n",
    "    return params\n",
    "\n",
    "def grads_to_vector(grads):\n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\"]:\n",
    "        new_vector = np.reshape(grads[key], (-1, 1))\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    J = theta * x\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x, theta):\n",
    "    dtheta = forward_propagation(x, theta) / theta\n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon=1e-7):\n",
    "    thetaplus = theta + epsilon\n",
    "    thetaminus = theta - epsilon\n",
    "    J_plus = forward_propagation(x, thetaplus)\n",
    "    J_minus = forward_propagation(x, thetaminus)\n",
    "    gradapprox = (J_plus - J_minus) / (2*epsilon)\n",
    "    \n",
    "    grad = backprop(x, theta)\n",
    "    \n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    \n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    if difference < 1e-7:\n",
    "        print(\"The gradient is correct!\")\n",
    "    else:\n",
    "        print(\"The gradient is wrong!\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct!\n",
      "difference = 2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-dimensional gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, Y, params):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = params[\"W1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    b1 = params[\"b2\"]\n",
    "    W3 = params[\"W3\"]\n",
    "    b3 = params[\"b3\"]\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    logprobs = np.multiply(Y, np.log(A3)) + np.multiply(1 - A3, np.log(1 - Y))\n",
    "    cost = (-1/m) * logprobs\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X, Y, cache):\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * np.dot(dZ3, A2.T)\n",
    "    db1 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ2)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\n",
    "        \"dZ3\":dZ3, \"dW3\":dW3, \"db3\":db3,\n",
    "        \"dZ2\":dZ2, \"dA2\":dA2, \"dW2\":dW2, \"db2\":db2,\n",
    "        \"dZ1\":dZ1, \"dA1\":dA1, \"dW1\":dW1, \"db1\":db1\n",
    "    }\n",
    "    \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(params, grads, X, Y, epsilon = 1e-7):\n",
    "    params_vectors, _ = dict_to_vector(params)\n",
    "    grads = grads_to_vector(grads)\n",
    "    num_params = params_vectors.shape[0]\n",
    "    J_plus = np.zeros((num_params, 1))\n",
    "    J_minus = np.zeros((num_params, 1))    \n",
    "    gradapprox = np.zeros((num_params, 1))\n",
    "    \n",
    "    for i in range(num_params):\n",
    "        thetaplus = np.copy(params_vectors)\n",
    "        thetaplus[i, 0] = thetaplus[i, 0] + epsilon\n",
    "        J_plus[i], _ = forward_propagation(X, Y, vector_to_dict(thetaplus))\n",
    "        \n",
    "        thetaminus = np.copy(params)\n",
    "        thetaminus[i, 0] = thetaminus[i, 0] - epsilon\n",
    "        J_minus[i], _ = forward_propagation(X, Y, vector_to_dict(thetaminus))\n",
    "        \n",
    "        gradapprox[i] = (J_plus - J_minus) / 2*epsilon\n",
    "        numerator = np.linalg.norm(grads[i] - gradapprox[i])\n",
    "        denominator = np.linalg.norm(grads[i]) + np.linalg.norm(gradapprox[i])\n",
    "        difference = numerator / denumerator\n",
    "        \n",
    "        if difference > 2e-7:\n",
    "            print (\"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
