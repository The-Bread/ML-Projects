{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_adam(params):\n",
    "    L = len(params)\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(params[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(params[\"b\" + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(params[\"W\" + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(params[\"b\" + str(l+1)].shape)\n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params_adam(params, grads, v, s, t, lr=0.01, beta1=0.9, beta2=0.999,\n",
    "                      epsilon=1e-8):\n",
    "    L = len(params)\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "        v_corrected[\"dW\" + str(l+1)] = d[\"dW\" + str(l+1)] / (1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l+1)] = d[\"db\" + str(l+1)] / (1 - beta1**t)\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * grads[\"dW\" + str(l+1)]\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "        params[\"W\" + str(l+1)] = params[\"W\" + str(l+1)] - lr*v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n",
    "        params[\"b\" + str(l+1)] = params[\"b\" + str(l+1)] - lr*v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n",
    "        \n",
    "    return params, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, params):\n",
    "    Wax = params[\"Wax\"]\n",
    "    Waa = params[\"Waa\"]\n",
    "    Wya = params[\"Wya\"]\n",
    "    ba  = params[\"ba\"]\n",
    "    by  = params[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
    "    \n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
    "    \n",
    "    cache = (a_next, a_prev, xt, params)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3, 10)\n",
    "a_prev = np.random.randn(5, 10)\n",
    "Waa = np.random.randn(5, 5)\n",
    "Wax = np.random.randn(5, 3)\n",
    "Wya = np.random.randn(2, 5)\n",
    "ba = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "params = {\"Waa\":Waa, \"Wax\":Wax, \"Wya\":Wya, \"ba\":ba, \"by\":by}\n",
    "\n",
    "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.59584544,  0.18141802,  0.61311866,  0.99808218,  0.85016201,\n",
       "         0.99980978, -0.18887155,  0.99815551,  0.6531151 ,  0.82872037]),\n",
       " (5, 10),\n",
       " array([0.9888161 , 0.01682021, 0.21140899, 0.36817467, 0.98988387,\n",
       "        0.88945212, 0.36920224, 0.9966312 , 0.9982559 , 0.17746526]),\n",
       " (2, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_next[4], a_next.shape, yt_pred[1], yt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de rnn_forward(x, a0, params):\n",
    "    caches = []\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = params[\"Wya\"].shape\n",
    "    \n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
